
# Rapport â€“ Laboratoire 4 â€“ LOG430

## Introduction

Dans ce laboratoire, j'avais pour objectif dâ€™ajouter des fonctionnalitÃ©s avancÃ©es Ã  mon systÃ¨me multi-magasins, en intÃ©grant :
- une couche d'observabilitÃ©,
- un mÃ©canisme d'Ã©quilibrage de charge,
- et un systÃ¨me de mise en cache applicatif.

Jâ€™ai mis en place des outils comme Prometheus, Grafana, Locust et Nginx, et jâ€™ai rÃ©alisÃ© des tests de charge pour Ã©valuer la robustesse du systÃ¨me.

---

## ObservabilitÃ© et Test de Charge

### IntÃ©gration Prometheus et Grafana

Jâ€™ai utilisÃ© la librairie `prometheus_fastapi_instrumentator` pour exposer les mÃ©triques sur `/metrics` (port 8004) de lâ€™interface FastAPI.

Parmi les mÃ©triques collectÃ©es :
- `http_requests_total` : nombre total de requÃªtes HTTP,
- `http_request_duration_seconds_sum` : somme des durÃ©es de traitement des requÃªtes,
- `process_resident_memory_bytes` : mÃ©moire utilisÃ©e,
- `python_gc_collections_total` : cycles de garbage collector.

Prometheus est configurÃ© pour interroger ces mÃ©triques toutes les 5 secondes avec la configuration suivante :

```yaml
scrape_configs:
  - job_name: "interface"
    static_configs:
      - targets: ["10.194.32.192:8004"]
```

Jâ€™ai lancÃ© Prometheus via Docker sur le port 9091. Ensuite, jâ€™ai connectÃ© Grafana Ã  Prometheus pour visualiser les mÃ©triques.

---

### Test de charge avec Locust

Jâ€™ai utilisÃ© Locust pour envoyer des requÃªtes sur les routes `/`, `/rapport` et `/performances`.

Avant test :
- Peu de requÃªtes,
- Faible latence (~22 ms Ã  104 ms),
- MÃ©moire utilisÃ©e : ~80 Mo.

Pendant test (6.7 RPS) :
- Ã‰checs 5xx sur toutes les routes testÃ©es,
- Latence en forte hausse : jusquâ€™Ã  13.4s de temps cumulÃ©,
- Utilisation CPU et mÃ©moire en augmentation.

ğŸ” **Conclusion** : Le systÃ¨me montre des failles sous charge, probablement dues Ã  des appels internes synchrones via `httpx`. Ces appels devront Ãªtre optimisÃ©s (timeouts, async, retry).

---

### Visualisation dans Grafana

Jâ€™ai ajoutÃ© les panels suivants :
- Nombre de requÃªtes HTTP,
- Temps de rÃ©ponse,
- Codes dâ€™erreurs (4xx, 5xx),
- Utilisation CPU/MÃ©moire.

Les courbes mâ€™ont permis dâ€™identifier l'impact des tests sur les performances du systÃ¨me.

---

## Ã‰quilibrage de Charge

Jâ€™ai mis en place un reverse proxy Nginx distribuant les requÃªtes entre deux instances de lâ€™interface FastAPI (ports 8004 et 8005).

Configuration `nginx.conf` :

```nginx
upstream backend {
    server interface1:8000;
    server interface2:8000;
}

server {
    listen 80;
    location / {
        proxy_pass http://backend;
    }
}
```

**RÃ©sultat** : Lâ€™interface rÃ©pond maintenant sur le port 8088. Les requÃªtes alternent bien entre les deux services selon le mode round-robin.

---

## Caching Applicatif

Jâ€™ai ajoutÃ© un cache Python simple pour amÃ©liorer la performance sur les requÃªtes frÃ©quentes, notamment `/produits`.

### Avant Caching (ApacheBench) :
- Temps moyen par requÃªte : 250 ms,
- 1000 requÃªtes, toutes avec code 500 (erreurs).

### AprÃ¨s Caching :
- Temps moyen rÃ©duit Ã  230 ms,
- Toujours 1000 requÃªtes, mais aucune requÃªte Ã©chouÃ©e.

ğŸ” **Conclusion** : Le cache a rÃ©duit lÃ©gÃ¨rement la latence et a stabilisÃ© les requÃªtes, bien que les rÃ©ponses soient toujours des erreurs (code 500), ce qui indique un autre bug fonctionnel ou une absence de gestion correcte des exceptions.

---

## Conclusion

Jâ€™ai atteint les objectifs suivants :
âœ… Mise en place dâ€™une observabilitÃ© complÃ¨te avec Prometheus et Grafana (UC7)  
âœ… RÃ©alisation de tests de charge avec Locust, puis ApacheBench  
âœ… ImplÃ©mentation dâ€™un Ã©quilibrage de charge avec Nginx (UC8)  
âœ… IntÃ©gration dâ€™un cache simple dans lâ€™application (UC9)  

GrÃ¢ce Ã  ces ajouts, mon systÃ¨me est plus robuste, observÃ© en temps rÃ©el, et scalable pour de futures Ã©volutions.
